{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/abhayyadav1074/dcgan?scriptVersionId=245099268\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## We will first import the required files. Note: You may have to install the libraries which are not currently installed. e.g.: imageio","metadata":{}},{"cell_type":"code","source":"pip install imageio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:39:51.63257Z","iopub.execute_input":"2025-06-12T14:39:51.632775Z","iopub.status.idle":"2025-06-12T14:39:57.672863Z","shell.execute_reply.started":"2025-06-12T14:39:51.632756Z","shell.execute_reply":"2025-06-12T14:39:57.671344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np \nimport math                     #data normalization\nimport matplotlib.pyplot as plt #plot images of outputs\nimport os                       # to read \nimport imageio                  # to create anim of outputs\nfrom sklearn import preprocessing #for standardizing the dataset\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:40:01.772194Z","iopub.execute_input":"2025-06-12T14:40:01.772517Z","iopub.status.idle":"2025-06-12T14:40:21.800433Z","shell.execute_reply.started":"2025-06-12T14:40:01.772483Z","shell.execute_reply":"2025-06-12T14:40:21.7996Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the Fashion-MNIST dataset\n\n## We are going to use the Fashion-MNIST dataset\n## Number of images: 60000\n## Dimensions: 28X28X1\n## This dataset has objects belonging to 10 classes.They are:\n## 0: T-shirt/top, 1: Trouser, 2: Pullover, 3: Dress, 4: Coat, 5: Sandal, 6: Shirt, 7: Sneaker, 8: Bag, 9: Ankle boot\n\n## We will first load the dataset, then visualize some sample images from the dataset","metadata":{}},{"cell_type":"code","source":"(x_train, y_train), (_, _) = tf.keras.datasets.fashion_mnist.load_data()\nprint(x_train.shape,y_train.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:40:32.25324Z","iopub.execute_input":"2025-06-12T14:40:32.253531Z","iopub.status.idle":"2025-06-12T14:40:32.76542Z","shell.execute_reply.started":"2025-06-12T14:40:32.253512Z","shell.execute_reply":"2025-06-12T14:40:32.76432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#plotting \nfor i in range(3):  #we plot 10 images\n    plt.subplot(1,3,1+i)   #2 rows, 5 cols, index of the image\n    plt.imshow(x_train[i*5], cmap=plt.get_cmap('gray'))\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:40:36.727299Z","iopub.execute_input":"2025-06-12T14:40:36.728287Z","iopub.status.idle":"2025-06-12T14:40:37.12581Z","shell.execute_reply.started":"2025-06-12T14:40:36.728256Z","shell.execute_reply":"2025-06-12T14:40:37.12483Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Building the Generator Network\n\n### Since the generator will up-sample the noise vector, we will use Conv2DTranspose module from Tensorflow. We create the network which performs the following transforms.\n\n### Input the Noise (100X1) to a Dense Layer (7X7X256) followed by series of up-sampling layers Conv2DTranspose to reach the desired image size (28X28X1)\n\n ","metadata":{}},{"cell_type":"code","source":"#Convert the train dataset into a 3D DATASET of stacked 3D images \nimage_width = x_train.shape[1];\nx_train = np.reshape(x_train, [-1, image_width ,image_width , 1]).astype('float32')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:40:42.602504Z","iopub.execute_input":"2025-06-12T14:40:42.602871Z","iopub.status.idle":"2025-06-12T14:40:42.713832Z","shell.execute_reply.started":"2025-06-12T14:40:42.602839Z","shell.execute_reply":"2025-06-12T14:40:42.713026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Check the shape of the data now\nx_train.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:40:48.177323Z","iopub.execute_input":"2025-06-12T14:40:48.17762Z","iopub.status.idle":"2025-06-12T14:40:48.18351Z","shell.execute_reply.started":"2025-06-12T14:40:48.1776Z","shell.execute_reply":"2025-06-12T14:40:48.182747Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### We will now standardize the data values. Currently the range of values is from 0 to 255. We may either modify it to be between 0 and 1 or between -1 to 1. If we intend to use the tanh activation function in the last layer, we may use the range -1 to 1 or use the range 0 to 1 for sigmoid activation in the output layer\n### Now, let use standardize the values","metadata":{}},{"cell_type":"code","source":"# Normalize the images to [-1, 1] [Normalized Value = (value - mean)/mean]\nx_train = (x_train - 127.5) / 127.5  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:40:53.022306Z","iopub.execute_input":"2025-06-12T14:40:53.022629Z","iopub.status.idle":"2025-06-12T14:40:53.153504Z","shell.execute_reply.started":"2025-06-12T14:40:53.022606Z","shell.execute_reply":"2025-06-12T14:40:53.152371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Generator Initializations\nnoise_length = 100\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:40:57.627407Z","iopub.execute_input":"2025-06-12T14:40:57.628137Z","iopub.status.idle":"2025-06-12T14:40:57.631965Z","shell.execute_reply.started":"2025-06-12T14:40:57.628108Z","shell.execute_reply":"2025-06-12T14:40:57.631021Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### We will use the Tensorflow functional API to build our network. By default the function considers a noise input size of 100, and image size of 28. Both parameters can be customized when the function is called.\n### We manipulate the stride in order to increase the dimensions of the input from 7 to 28 and maniputate the filter count to downsize the output to 1 channel in the last layer.","metadata":{}},{"cell_type":"code","source":"def create_generator_model(image_size=28, noise_input=100):\n   \n   #Create input layer\n   input_layer = tf.keras.layers.Input(shape=(noise_input,))\n   \n   #First upsampling to 7X7X256 \n   #Increase dimensions and resize to 3D to feed it to Conv2DTranspose layer\n   x = tf.keras.layers.Dense(7 * 7 * 256)(input_layer)\n   x = tf.keras.layers.Reshape((7, 7, 256))(x)\n   \n   #Upscaling 1 : 128 filters, (2,2) stride\n   #Input = 7X7X256\n   #Output at this stage = 14X14X128\n   #Syntax Note: kernel size can be specified as a tuple or integer or list\n   x = tf.keras.layers.BatchNormalization()(x)\n   x = tf.keras.layers.Activation('leaky_relu')(x)\n   x = tf.keras.layers.Conv2DTranspose(128, (5,5), strides=2, padding='same')(x)\n   \n   #Upscaling 2\n   #Input = 14X14X128\n   #Output at this stage = 28X28X64\n   x = tf.keras.layers.BatchNormalization()(x)\n   x = tf.keras.layers.Activation('leaky_relu')(x)\n   x = tf.keras.layers.Conv2DTranspose(64, kernel_size=[5,5], strides=2, padding='same')(x)\n   \n   #Upscaling 3\n   #Input = 28X28X64\n   #Output at this stage = 28X28X32\n   x = tf.keras.layers.BatchNormalization()(x)\n   x = tf.keras.layers.Activation('leaky_relu')(x)\n   x = tf.keras.layers.Conv2DTranspose(32, kernel_size=[5,5], strides=1, padding='same')(x)\n   \n   #Upscaling 3\n   #Input = 28X28X32\n   #Output at this stage = 28X28X1\n   #Note that we use tanh activation and not Leaky Relu for the last layer. \n   x = tf.keras.layers.BatchNormalization()(x)\n   x = tf.keras.layers.Activation('leaky_relu')(x)\n   x = tf.keras.layers.Conv2DTranspose(1, kernel_size=[5,5], strides=1, padding='same')(x)\n   \n   x = tf.keras.layers.Activation('sigmoid')(x)\n   #Create the model    \n   gen_network = tf.keras.models.Model(input_layer, x, name='gen_network')\n   \n   return gen_network\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:41:02.053392Z","iopub.execute_input":"2025-06-12T14:41:02.053753Z","iopub.status.idle":"2025-06-12T14:41:02.062784Z","shell.execute_reply.started":"2025-06-12T14:41:02.053726Z","shell.execute_reply":"2025-06-12T14:41:02.061822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_discriminator_model(desc_data=[28,28,1,]):\n   \n   #A typical Convolution  network for classification is built  \n   disc_input = tf.keras.layers.Input(desc_data)\n   #Input Dimension : 28X28X1\n   #Output Dimension : 14X14X32\n   x = tf.keras.layers.LeakyReLU(alpha=0.2)(disc_input)\n   x = tf.keras.layers.Conv2D(32, kernel_size=[5,5], strides=2, padding='same')(x)\n   \n   #Input Dimension : 14X14X32\n   #Output Dimension : 7X7X64\n   x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n   x = tf.keras.layers.Conv2D(64, kernel_size=[5,5], strides=2, padding='same')(x)\n   \n   #Input Dimension : 7X7X64\n   #Output Dimension : 4X4X128\n   x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n   x = tf.keras.layers.Conv2D(128, kernel_size=[5,5], strides=2, padding='same')(x)\n   \n   #Input Dimension : 4X4X128\n   #Output Dimension : 4X4X256\n   x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n   x = tf.keras.layers.Conv2D(256, kernel_size=[5,5], strides=1, padding='same')(x)\n   \n   #Input Dimension : Flattened(4X4X256)\n   #Output Dimension : 1\n   #Flatten the output and build an output layer\n   x = tf.keras.layers.Flatten()(x)\n   x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n   \n   #Build Model\n   disc_network = tf.keras.models.Model(disc_input, x, name='disc_network')\n   \n   return disc_network\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:41:06.327269Z","iopub.execute_input":"2025-06-12T14:41:06.327627Z","iopub.status.idle":"2025-06-12T14:41:06.33535Z","shell.execute_reply.started":"2025-06-12T14:41:06.327601Z","shell.execute_reply":"2025-06-12T14:41:06.334191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator = create_generator_model()\nnoise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)\nplt.imshow(generated_image[0, :, :, 0], cmap='gray')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:41:10.71831Z","iopub.execute_input":"2025-06-12T14:41:10.71863Z","iopub.status.idle":"2025-06-12T14:41:11.217379Z","shell.execute_reply.started":"2025-06-12T14:41:10.718608Z","shell.execute_reply":"2025-06-12T14:41:11.21643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#This code will check the functioning of the descriminator against a sample input. As of this point, \n#the descriminator is untrained.\ndiscriminator = create_discriminator_model()\ndecision = discriminator(generated_image)\nprint (decision)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:41:15.426041Z","iopub.execute_input":"2025-06-12T14:41:15.426354Z","iopub.status.idle":"2025-06-12T14:41:15.520231Z","shell.execute_reply.started":"2025-06-12T14:41:15.426334Z","shell.execute_reply":"2025-06-12T14:41:15.519275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_models():\n   \n   noise_size = 100\n   lr = 2e-4\n   decay = 6e-8\n   \n   #Build Base Discriminator model\n   base_discriminator = create_discriminator_model(desc_data=(28,28,1,))\n   \n   #Define optimizer and compile model\n   discriminator = tf.keras.models.Model(inputs=base_discriminator.inputs, \n                                         outputs=base_discriminator.outputs)\n   optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n\n\n   discriminator.compile(loss='binary_crossentropy',\n                         optimizer=optimizer,\n                         metrics=['accuracy'])\n   \n   #Build Generator model\n   generator = create_generator_model(image_size=28, noise_input=noise_size)\n   \n   #Build Frozen Discriminator\n   frozen_discriminator = tf.keras.models.Model(inputs=base_discriminator.inputs, \n                                         outputs=base_discriminator.outputs)\n   #Freeze the weights of discriminator during adversarial training\n   frozen_discriminator.trainable = False\n   #Build Adversarial model\n   optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n\n   #Adversarial = generator + discriminator\n   adversarial = tf.keras.models.Model(generator.input, \n                       frozen_discriminator(generator.output))\n   \n   adversarial.compile(loss='binary_crossentropy',\n                       optimizer=optimizer,\n                       metrics=['accuracy'])    \n   \n   return generator, discriminator, adversarial\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:41:22.064127Z","iopub.execute_input":"2025-06-12T14:41:22.064422Z","iopub.status.idle":"2025-06-12T14:41:22.071543Z","shell.execute_reply.started":"2025-06-12T14:41:22.064402Z","shell.execute_reply":"2025-06-12T14:41:22.070483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_gan(generator, discriminator, adversarial, noise_size=100, train_steps=2000, batch_size=32, preview_interval=200):\n    import time\n    import tensorflow as tf\n    import numpy as np\n\n    image_size = 28\n\n    # Load Fashion MNIST dataset\n    (x_train, _), (_, _) = tf.keras.datasets.fashion_mnist.load_data()\n\n    # Preprocess: reshape and normalize\n    x_train = np.reshape(x_train, [-1, image_size, image_size, 1]).astype('float32') / 255.0\n\n    # Create test noise input for visual inspection\n    test_noise_input = np.random.uniform(-1.0, 1.0, size=[16, noise_size])\n\n    print(f\"Starting training for {train_steps} steps with batch size {batch_size}...\\n\")\n    start_time = time.time()\n\n    for i in range(train_steps):\n        # === Train Discriminator ===\n        noise_input = np.random.uniform(-1.0, 1.0, size=[batch_size, noise_size])\n        fake_images = generator.predict(noise_input, verbose=0)\n\n        real_images = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n\n        X = np.concatenate((real_images, fake_images))\n        y_real = np.ones((batch_size, 1)) * 0.9\n\n        y_fake = np.zeros((batch_size, 1))\n        y = np.concatenate((y_real, y_fake))\n\n        d_loss, d_acc = discriminator.train_on_batch(X, y)\n\n        # === Train Adversarial (Generator through combined model) ===\n        noise_input = np.random.uniform(-1.0, 1.0, size=[batch_size, noise_size])\n        y_adv = np.ones((batch_size, 1))\n\n        a_loss, a_acc = adversarial.train_on_batch(noise_input, y_adv)\n\n        # Print status\n        if i % 100 == 0:\n            print(f\"{i} [D loss: {d_loss:.6f}, acc: {d_acc:.6f}, A loss: {a_loss:.6f}, acc: {a_acc:.6f}]\")\n\n        # Save preview images\n        if (i + 1) % preview_interval == 0:\n            fake_images = generator.predict(test_noise_input, verbose=0)\n            plot_images(fake_images, i + 1)\n\n    elapsed = time.time() - start_time\n    print(f\"\\nTraining completed in {elapsed:.2f} seconds.\")\n\n    # Save final generator model\n    generator.save('fashionmnist_generator_dcgan_fast.h5')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T15:09:38.626426Z","iopub.execute_input":"2025-06-12T15:09:38.626699Z","iopub.status.idle":"2025-06-12T15:09:38.63854Z","shell.execute_reply.started":"2025-06-12T15:09:38.626678Z","shell.execute_reply":"2025-06-12T15:09:38.637769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_images(fake_images, step):\n   \n   plt.figure(figsize=(2.5,2.5))\n   num_images = fake_images.shape[0]\n   \n   image_size = fake_images.shape[1]\n   rows = int(math.sqrt(fake_images.shape[0]))\n   \n   for i in range(num_images):\n       plt.subplot(rows, rows, i + 1)\n       image = np.reshape(fake_images[i], [image_size, image_size])\n       plt.imshow(image, cmap='gray')\n       plt.axis('off')\n   plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:41:32.21239Z","iopub.execute_input":"2025-06-12T14:41:32.213372Z","iopub.status.idle":"2025-06-12T14:41:32.219218Z","shell.execute_reply.started":"2025-06-12T14:41:32.213341Z","shell.execute_reply":"2025-06-12T14:41:32.218066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"G, D, A = build_models()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:41:36.657228Z","iopub.execute_input":"2025-06-12T14:41:36.657576Z","iopub.status.idle":"2025-06-12T14:41:36.823378Z","shell.execute_reply.started":"2025-06-12T14:41:36.657548Z","shell.execute_reply":"2025-06-12T14:41:36.822392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"G.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:41:40.322387Z","iopub.execute_input":"2025-06-12T14:41:40.322729Z","iopub.status.idle":"2025-06-12T14:41:40.350085Z","shell.execute_reply.started":"2025-06-12T14:41:40.322705Z","shell.execute_reply":"2025-06-12T14:41:40.3492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"A.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:41:45.842597Z","iopub.execute_input":"2025-06-12T14:41:45.843008Z","iopub.status.idle":"2025-06-12T14:41:45.872174Z","shell.execute_reply.started":"2025-06-12T14:41:45.842979Z","shell.execute_reply":"2025-06-12T14:41:45.871481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"D.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:41:51.112344Z","iopub.execute_input":"2025-06-12T14:41:51.112647Z","iopub.status.idle":"2025-06-12T14:41:51.135839Z","shell.execute_reply.started":"2025-06-12T14:41:51.112627Z","shell.execute_reply":"2025-06-12T14:41:51.135162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_gan(G, D, A)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:50:23.777214Z","iopub.execute_input":"2025-06-12T14:50:23.777859Z","iopub.status.idle":"2025-06-12T15:09:38.62479Z","shell.execute_reply.started":"2025-06-12T14:50:23.777828Z","shell.execute_reply":"2025-06-12T15:09:38.623824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport imageio\n\nanim_file = 'dcfashiongan.gif'\n\nfilenames = glob.glob(r'D:\\Sups\\Python\\GAN\\FASHION_mnist\\*.png')\nfilenames = sorted(filenames)\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n    for filename in filenames:\n        image = imageio.imread(filename)\n        writer.append_data(image)\n    \n    # Append the last image again\n    if filenames:\n        last_image = imageio.imread(filenames[-1])\n        writer.append_data(last_image)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T15:12:28.022282Z","iopub.execute_input":"2025-06-12T15:12:28.022612Z","iopub.status.idle":"2025-06-12T15:12:28.03069Z","shell.execute_reply.started":"2025-06-12T15:12:28.022588Z","shell.execute_reply":"2025-06-12T15:12:28.02974Z"}},"outputs":[],"execution_count":null}]}